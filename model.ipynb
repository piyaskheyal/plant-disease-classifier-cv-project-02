{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57157d21",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb299f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"emmarex/plantdisease\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e170e8c",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to a class folder in your training data\n",
    "class_folder = 'dataset_split/train/Pepper__bell___Bacterial_spot'\n",
    "image_name = os.listdir(class_folder)[0]  # just pick the first image\n",
    "image_path = os.path.join(class_folder, image_name)\n",
    "\n",
    "# Open the image using PIL\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Analyze\n",
    "print(f\"Mode: {img.mode}\")         # e.g. 'RGB'\n",
    "print(f\"Size: {img.size}\")         # e.g. (256, 256)\n",
    "print(f\"Format: {img.format}\")     # e.g. 'JPEG'\n",
    "\n",
    "# Display\n",
    "plt.imshow(img)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),               # Resize smaller edge to 256\n",
    "    transforms.CenterCrop(224),           # Crop 224x224 square from center\n",
    "    # transforms.ToTensor(),                # Convert PIL image to tensor (C x H x W), values [0,1]\n",
    "    transforms.Normalize(                 # Normalize with ImageNet mean/std\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data in tensors\n",
    "training_dataset = ImageFolder(root=\"dataset_split/train\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=32,      # You can tune this\n",
    "    shuffle=True,       # Shuffle for training\n",
    "    num_workers=2       # Optional: use 2 subprocesses to load data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet18\n",
    "default_weights = ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=default_weights)\n",
    "\n",
    "# Freeze all layers (feature extractor part)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final fully connected layer\n",
    "num_classes = 15  # your case\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab75b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # only train final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11125ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')  # or 'cuda' if available in future\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ImageFolder(root=\"dataset_split/val\", transform=transform)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce50844",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)  # batch loss * batch size\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total * 100\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / total\n",
    "    val_acc = correct / total * 100\n",
    "\n",
    "    # ---------- Output ----------\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c891c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (only the weights)\n",
    "torch.save(model.state_dict(), \"plant_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8ce0c",
   "metadata": {},
   "source": [
    "# Prediction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74d7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
